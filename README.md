
# Qbrian â€” Quantum Neural Network Building Blocks

> **Qbrian** (read â€œQ-brainâ€) is a modular, batteries-included library of **quantum and hybrid quantumâ€“classical neural network blocks** for vision and sequence learning. It focuses on **clean APIs**, **robust fallbacks**, and **reproducible results**.

This repository groups multiple building blocksâ€”attention, CNN/Vision, transformers, normalization,
positional encodings, patch ops, and moreâ€”implemented with **Qiskit**, **PennyLane**, and **PyTorch**.
All modules follow **DRY/KISS/SOLID**, include **type hints**, **docstrings**, region markers, and avoid
runtime print noise (favoring return values and logging).

> Naming note: per the project brief, any former `coordinate_attention` symbols appear as `cordinate_attention`.

---

## Highlights

- **Attention family**: CBAM, cross/self/masked, window-based Q self-attention, *qslot_attention*, *cordinate_attention*, QRAMâ€‘enhanced selfâ€‘attention, QSANN stack.
- **Vision family**: Quantum MobileNet, Quantum Convolution (amplitude encoding), Quantum Squeezeâ€‘andâ€‘Excitation (QSE), QGhost, ViTâ€‘style patch embedding (QViT), quantum patch merging/splitting.
- **Positional/Embedding**: QPE (quantum positional encoding), **qcombined_encoding** (PennyLane rotation + amplitude), **Qhierarchical_embedding** (QHEB).
- **Transformer blocks**: Quantum Transformer **Encoder** and **Decoder** helpers.
- **Normalization**: QuantumLayerNormalization (analytical + variational variants).
- **Engineering**: graceful fallbacks when `qiskit-machine-learning`/**PennyLane** are missing; explicit shape checks; minimal side effects.



## ğŸ“¦ Installation

> **Python**: 3.9â€“3.11 recommended

Install system build tools (Linux/macOS):
```bash
# Linux (Debian/Ubuntu)
sudo apt-get update && sudo apt-get install -y build-essential python3-dev

# macOS (Homebrew)
brew install cmake
```

Create a virtualenv and install Python requirements:
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip

# If you have the project's requirements.txt:
pip install -r requirements.txt
```

### Backend notes
- `qiskit-aer` provides the simulators used in most modules (`AerSimulator`, `statevector_simulator`, etc.).
- `qiskit-machine-learning` enables `EstimatorQNN` + `TorchConnector`. Where itâ€™s unavailable, modules providing fallbacks will still run (with shaped random outputs) so your pipelines donâ€™t break.
- `pennylane` is used by **qcombined_encoding**; if itâ€™s missing, a small MLP fallback is used.
- GPU support is optional; most examples run on CPU-only.

---

## Why Quantum Neural Networks?

Quantum models arenâ€™t a silver bulletâ€”but they offer unique capabilities that can complement or extend classical deep learning:

- **Expressive feature spaces via quantum embeddings**  
  Parameterized quantum circuits (PQCs) can encode data into **high-dimensional Hilbert spaces** with entanglement and interference, enabling kernel spaces that may be *hard to simulate classically* in certain regimes.

- **Entanglement as structured inductive bias**  
  Controlled entangling operations create **explicit cross-feature interactions** at shallow depths. This acts like a learned multiplicative coupling across channels/tokens without large parameter counts.

- **Compact parameterization**  
  Short-depth variational circuits can represent rich functions with **fewer trainable parameters**, potentially lowering memory and overfitting risk for small/medium datasets.

- **Data re-uploading & reusability**  
  Cyclic â€œencodeâ€“entangleâ€“encodeâ€ patterns let models **revisit inputs across layers**, improving approximation capacity without very deep classical towers.

- **Hybrid learning stability**  
  Qbrianâ€™s blocks are designed for **hybrid training** (classical optimizers driving quantum layers), with practical choicesâ€”local costs, shallow circuits, and measured observablesâ€”to help **mitigate barren plateaus** in realistic settings.

- **Probabilistic inference for free**  
  Measurement yields distributions naturally. You can leverage sampling strategies for **uncertainty estimation**, calibration, and stochastic regularization without extra machinery.

- **Potential quantum advantage settings**  
  While problemâ€‘dependent and hardwareâ€‘limited today, tasks involving **structured linear algebra, kernel methods, or specific combinatorial structures** may benefit from quantum subroutines embedded in endâ€‘toâ€‘end systems.

> Reality check: current advantages are **contextual** and depend on data, circuit design, and hardware. Qbrian emphasizes **transparent APIs and fallbacks** so you can explore these regimes without blocking on a particular stack.

---

## Package Map (high level)

```
qbrian/
â”œâ”€â”€ cbam/                         # Quantum CBAM
â”œâ”€â”€ cordinate_attention/          # â€œcoordinateâ€ attention (intentional name)
â”œâ”€â”€ cross_attention/
â”œâ”€â”€ masked_self_attention/
â”œâ”€â”€ qffn/                         # FastQuantumLayer + OptimizedQuantumFFN
â”œâ”€â”€ qpe/                          # Quantum Positional Encoding
â”œâ”€â”€ qram_selfattention/           # WorkingQRAM, QRAM_Layer, QRAM_Attention, QRAM_QSANN_Working
â”œâ”€â”€ qghost/                       # Quantum Ghost block
â”œâ”€â”€ qlayer_normalization/         # QuantumLayerNormalization (+ variational)
â”œâ”€â”€ qmobilenet/                   # Quantum MobileNet & blocks
â”œâ”€â”€ qpatch_merging_splitting/     # Patch merging/splitting circuits
â”œâ”€â”€ qse/                          # Quantum Squeeze-and-Excitation
â”œâ”€â”€ qtransformer_decoder/         # Decoder helpers (QNN + hybrid head)
â”œâ”€â”€ qtransformer_encoder/         # Encoder helpers (attention + FFN)
â”œâ”€â”€ qvit/                         # ViT-style patch embedding
â”œâ”€â”€ qslot_attention/              # renamed slot attention
â”œâ”€â”€ window_based_qattention/      # window-based quantum self-attention
â”œâ”€â”€ qconv/                        # Quantum convolution (amplitude encoding)
â”œâ”€â”€ qresnet/                      # Quantum ResNet architecture & blocks
â”œâ”€â”€ qsann/                        # Stacked quantum self-attention network
â”œâ”€â”€ qcombined_encoding/           # PennyLane rotation + amplitude
â””â”€â”€ qhierarchical_embedding/      # QHEB feature map (+ optional VQC factory)
```

Each module ships with docstrings, type hints, andâ€”where helpfulâ€”minimal example scripts under `examples/`.

---

## Design Principles

- **Pragmatic research ergonomics**: quantum where it helps, **classical fallbacks** elsewhere.  
- **DRY / KISS / SOLID**: small modules, explicit dependencies, testable units.  
- **Backâ€‘compat awareness**: guards and multiâ€‘path imports for shifting Qiskit APIs.  
- **Zero sideâ€‘effect printing**: core returns values; you own logging/UX.

---

## Practical Notes & Caveats

- **Simulation cost** grows with qubit count and depth; start small and scale thoughtfully.  
- **Version compatibility** mattersâ€”quantum stacks evolve quickly; Qbrian adds guards/fallbacks, but pinning versions for experiments is wise.  
- **Evaluation** should include **classical baselines**; measure accuracy *and* sample/latency/energy where relevant.

---

## Contributing

Issues and PRs are welcome. Please include:
- Minimal repro or unit test
- Environment info (OS, Python, package versions)
- Expected vs. actual behavior

Follow standard formatting/typing before submitting.

---

## Citation

If you use Qbrian in academic work:

```bibtex
@software{qbrian2025,
  title   = {Qbrian: Quantum Neural Network Building Blocks},
  year    = {2025},
  url     = {https://github.com/Q-brain/Qbrian}
}
```

---

## License

Add your chosen license (e.g., MIT) as `LICENSE` at the repo root.

---

## Acknowledgments

Grateful to the **Qiskit** and **PennyLane** communities for the foundations that make hybrid quantum research accessible.
